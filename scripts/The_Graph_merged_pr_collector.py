import sys
import os

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import requests
import pandas as pd
import json
import re
from datetime import datetime
from config.settings_template import GITHUB_TOKEN, THEGRAPH_CONFIG


class TheGraphContractsCollector:
    def __init__(self):
        self.headers = {'Authorization': f'token {GITHUB_TOKEN}'}
        self.owner = THEGRAPH_CONFIG['owner']
        self.repo = THEGRAPH_CONFIG['repo']
        self.base_url = f"https://api.github.com/repos/{self.owner}/{self.repo}"

        # é€šç”¨bugç›¸å…³å…³é”®è¯ï¼ˆä¸åŸç ”ç©¶ä¿æŒä¸€è‡´ï¼‰
        self.general_bug_keywords = [
            'bug', 'fix', 'repair', 'defect', 'vulnerability', 'issue',
            'error', 'problem', 'incorrect', 'wrong', 'fail', 'crash',
            'security', 'exploit', 'attack', 'overflow', 'underflow',
            'reentrancy', 'gas', 'optimization', 'revert', 'panic'
        ]

        # The Graph Infrastructureç‰¹å®šå…³é”®è¯
        self.thegraph_keywords = [
            'subgraph', 'indexer', 'curator', 'delegator', 'indexing',
            'query', 'grt', 'token', 'stake', 'staking', 'delegate',
            'delegation', 'slash', 'slashing', 'reward', 'rebate',
            'allocation', 'signal', 'curation', 'thaw', 'collect',
            'withdraw', 'deposit', 'channel', 'attestation', 'dispute',
            'fisherman', 'epoch', 'settlement', 'payment', 'gateway',
            'subgraphdeployment', 'metadata', 'ipfs', 'graphql',
            'manifest', 'schema', 'mapping', 'datasource', 'handler',
            'entity', 'store', 'block', 'transaction', 'receipt',
            'log', 'event', 'call', 'function', 'contract'
        ]

        # åˆå¹¶æ‰€æœ‰å…³é”®è¯
        self.bug_keywords = self.general_bug_keywords + self.thegraph_keywords

        self.merged_prs = []

    def collect_all_merged_prs(self):
        """æ”¶é›†æ‰€æœ‰å·²åˆå¹¶çš„PR"""
        print("ğŸ“¥ æ­£åœ¨æ”¶é›†The Graph Contractsæ‰€æœ‰å·²åˆå¹¶çš„PR...")
        print(f"ğŸ”— ä»“åº“: {self.owner}/{self.repo}")

        merged_prs = []
        page = 1
        total_collected = 0

        while True:
            print(f"   æ­£åœ¨è·å–ç¬¬ {page} é¡µ...")

            # åªè·å–mergedçŠ¶æ€çš„PR
            prs = self.make_request(f"{self.base_url}/pulls", {
                'state': 'closed',  # GitHub API: closedåŒ…å«mergedå’Œæœªmergedçš„
                'per_page': 100,
                'page': page,
                'sort': 'updated',
                'direction': 'desc'
            })

            if not prs:
                break

            # ç­›é€‰å‡ºçœŸæ­£mergedçš„PR
            page_merged_count = 0
            for pr in prs:
                if pr.get('merged_at') is not None:  # å…³é”®ï¼šåªè¦merged_atä¸ä¸ºç©º
                    merged_prs.append({
                        'project_name': 'The Graph',
                        'project_type': 'Infrastructure',
                        'project_domain': 'Indexing Protocol',
                        'number': pr['number'],
                        'title': pr['title'],
                        'body': pr.get('body', '') or '',
                        'state': pr['state'],
                        'merged_at': pr['merged_at'],
                        'created_at': pr['created_at'],
                        'user': pr['user']['login'],
                        'url': pr['html_url'],
                        'labels': [label['name'] for label in pr.get('labels', [])],
                        'commits': pr.get('commits', 0),
                        'additions': pr.get('additions', 0),
                        'deletions': pr.get('deletions', 0),
                        'changed_files': pr.get('changed_files', 0),
                        'assignees': [assignee['login'] for assignee in pr.get('assignees', [])],
                        'milestone': pr.get('milestone', {}).get('title', '') if pr.get('milestone') else '',
                        'base_ref': pr.get('base', {}).get('ref', ''),
                        'head_ref': pr.get('head', {}).get('ref', '')
                    })
                    page_merged_count += 1

            total_collected += page_merged_count
            print(f"   ç¬¬ {page} é¡µæ‰¾åˆ° {page_merged_count} ä¸ªåˆå¹¶çš„PR (æ€»è®¡: {total_collected})")

            # å¦‚æœè¿™ä¸€é¡µæ²¡æœ‰mergedçš„PRï¼Œå¯èƒ½å·²ç»åˆ°åº•äº†
            if page_merged_count == 0:
                break

            page += 1

        print(f"âœ… æ€»å…±æ”¶é›†åˆ° {len(merged_prs)} ä¸ªå·²åˆå¹¶çš„PR")
        return merged_prs

    def analyze_merged_prs(self, merged_prs):
        """åˆ†æå·²åˆå¹¶çš„PR"""
        print("ğŸ“Š åˆ†æThe Graph Contractså·²åˆå¹¶çš„PR...")

        # åŸºæœ¬ç»Ÿè®¡
        total_prs = len(merged_prs)

        # æ—¶é—´åˆ†æ
        dates = [pr['merged_at'][:10] for pr in merged_prs]
        date_counts = pd.Series(dates).value_counts().sort_index()

        # ç”¨æˆ·åˆ†æ
        users = [pr['user'] for pr in merged_prs]
        user_counts = pd.Series(users).value_counts()

        # æ ‡ç­¾åˆ†æ
        all_labels = []
        for pr in merged_prs:
            all_labels.extend(pr['labels'])
        label_counts = pd.Series(all_labels).value_counts()

        # ä»£ç å˜æ›´åˆ†æ
        total_additions = sum(pr['additions'] for pr in merged_prs)
        total_deletions = sum(pr['deletions'] for pr in merged_prs)
        total_files = sum(pr['changed_files'] for pr in merged_prs)

        # The Graphç‰¹å®šåˆ†æ
        staking_keywords = ['stake', 'staking', 'delegate', 'delegation', 'slash', 'slashing']
        curation_keywords = ['curator', 'curation', 'signal', 'grt', 'token']
        indexing_keywords = ['indexer', 'indexing', 'subgraph', 'query', 'allocation']

        staking_prs = [pr for pr in merged_prs
                       if any(keyword in pr['title'].lower() or keyword in pr['body'].lower()
                              for keyword in staking_keywords)]

        curation_prs = [pr for pr in merged_prs
                        if any(keyword in pr['title'].lower() or keyword in pr['body'].lower()
                               for keyword in curation_keywords)]

        indexing_prs = [pr for pr in merged_prs
                        if any(keyword in pr['title'].lower() or keyword in pr['body'].lower()
                               for keyword in indexing_keywords)]

        print(f"ğŸ“ˆ The Graph Contractsç»Ÿè®¡ç»“æœ:")
        print(f"   - æ€»åˆå¹¶PRæ•°: {total_prs}")
        print(f"   - Stakingç›¸å…³PRæ•°: {len(staking_prs)}")
        print(f"   - Curationç›¸å…³PRæ•°: {len(curation_prs)}")
        print(f"   - Indexingç›¸å…³PRæ•°: {len(indexing_prs)}")
        print(f"   - æœ€æ—©åˆå¹¶æ—¥æœŸ: {min(dates) if dates else 'N/A'}")
        print(f"   - æœ€æ™šåˆå¹¶æ—¥æœŸ: {max(dates) if dates else 'N/A'}")
        print(
            f"   - æœ€æ´»è·ƒè´¡çŒ®è€…: {user_counts.head(1).index[0] if not user_counts.empty else 'N/A'} ({user_counts.iloc[0] if not user_counts.empty else 0} PRs)")
        print(f"   - æ€»ä»£ç è¡Œå˜æ›´: +{total_additions:,} -{total_deletions:,}")
        print(f"   - æ€»æ–‡ä»¶å˜æ›´: {total_files:,}")

        return {
            'total_prs': total_prs,
            'staking_prs': len(staking_prs),
            'curation_prs': len(curation_prs),
            'indexing_prs': len(indexing_prs),
            'date_counts': date_counts,
            'user_counts': user_counts,
            'label_counts': label_counts,
            'code_stats': {
                'additions': total_additions,
                'deletions': total_deletions,
                'files': total_files
            }
        }

    def identify_bug_fix_prs(self, merged_prs):
        """ä»å·²åˆå¹¶çš„PRä¸­è¯†åˆ«bugä¿®å¤ç›¸å…³çš„PRï¼ˆéµå¾ªåŸç ”ç©¶æ–¹æ³•è®ºï¼‰"""
        print("ğŸ” è¯†åˆ«The Graph Contracts bugä¿®å¤ç›¸å…³çš„PR...")

        bug_candidates = []

        for pr in merged_prs:
            title_lower = pr['title'].lower()
            body_lower = pr['body'].lower()
            labels_lower = [label.lower() for label in pr['labels']]

            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            title_body_text = title_lower + ' ' + body_lower

            # é€šç”¨bugå…³é”®è¯åŒ¹é…
            general_keyword_matches = [kw for kw in self.general_bug_keywords if kw in title_body_text]

            # The Graphç‰¹å®šå…³é”®è¯åŒ¹é…
            thegraph_keyword_matches = [kw for kw in self.thegraph_keywords if kw in title_body_text]

            # æ£€æŸ¥æ ‡ç­¾
            bug_labels = ['bug', 'defect', 'security', 'vulnerability', 'fix', 'hotfix', 'patch', 'critical']
            label_matches = [label for label in labels_lower if any(bug_label in label for bug_label in bug_labels)]

            # æ£€æŸ¥fixå¼•ç”¨æ¨¡å¼
            fix_patterns = [
                r'fix(?:es)?\s*#?\d+',  # fixes #123
                r'resolv(?:es)?\s*#?\d+',  # resolves #123
                r'clos(?:es)?\s*#?\d+',  # closes #123
                r'fix(?:es)?\s+\w+',  # fixes bug
                r'patch(?:es)?\s+\w+',  # patches issue
            ]
            fix_references = []
            for pattern in fix_patterns:
                fix_references.extend(re.findall(pattern, title_body_text))

            # The Graphç‰¹å®šçš„bugæ¨¡å¼
            thegraph_bug_patterns = [
                r'staking.*(?:fail|error|bug|incorrect|slash)',
                r'delegation.*(?:fail|error|bug|wrong|invalid)',
                r'curation.*(?:fail|error|bug|signal|manipulation)',
                r'indexer.*(?:fail|error|bug|slash|penalty)',
                r'subgraph.*(?:fail|error|bug|invalid|malformed)',
                r'query.*(?:fail|error|bug|timeout|invalid)',
                r'allocation.*(?:fail|error|bug|overflow|underflow)',
                r'reward.*(?:fail|error|bug|calculation|distribution)',
                r'grt.*(?:fail|error|bug|transfer|burn|mint)',
                r'epoch.*(?:fail|error|bug|settlement|update)',
                r'channel.*(?:fail|error|bug|payment|dispute)',
                r'attestation.*(?:fail|error|bug|invalid|verification)'
            ]

            thegraph_bug_matches = []
            for pattern in thegraph_bug_patterns:
                thegraph_bug_matches.extend(re.findall(pattern, title_body_text))

            # è®¡ç®—åŒ¹é…åˆ†æ•°ï¼ˆä¸åŸç ”ç©¶æ–¹æ³•è®ºä¸€è‡´ï¼‰
            match_score = (len(general_keyword_matches) +
                           len(label_matches) +
                           len(fix_references) +
                           len(thegraph_bug_matches))

            if general_keyword_matches or label_matches or fix_references or thegraph_bug_matches:
                confidence = 'high' if match_score >= 3 else 'medium' if match_score >= 1 else 'low'

                bug_candidates.append({
                    **pr,
                    'general_keyword_matches': general_keyword_matches,
                    'thegraph_keyword_matches': thegraph_keyword_matches,
                    'label_matches': label_matches,
                    'fix_references': fix_references,
                    'thegraph_bug_matches': thegraph_bug_matches,
                    'match_score': match_score,
                    'confidence': confidence
                })

        print(f"âœ… ä» {len(merged_prs)} ä¸ªåˆå¹¶PRä¸­è¯†åˆ«å‡º {len(bug_candidates)} ä¸ªç–‘ä¼¼bugä¿®å¤PR")

        # æŒ‰ç½®ä¿¡åº¦åˆ†ç±»ç»Ÿè®¡
        high_confidence = len([c for c in bug_candidates if c['confidence'] == 'high'])
        medium_confidence = len([c for c in bug_candidates if c['confidence'] == 'medium'])
        low_confidence = len([c for c in bug_candidates if c['confidence'] == 'low'])

        print(f"   - é«˜ç½®ä¿¡åº¦: {high_confidence}")
        print(f"   - ä¸­ç½®ä¿¡åº¦: {medium_confidence}")
        print(f"   - ä½ç½®ä¿¡åº¦: {low_confidence}")

        # æŒ‰The GraphåŠŸèƒ½åˆ†ç±»ç»Ÿè®¡
        staking_bugs = len(
            [c for c in bug_candidates if any('staking' in match or 'delegation' in match or 'slash' in match
                                              for match in c['thegraph_keyword_matches'] + c['thegraph_bug_matches'])])
        curation_bugs = len(
            [c for c in bug_candidates if any('curation' in match or 'signal' in match or 'curator' in match
                                              for match in c['thegraph_keyword_matches'] + c['thegraph_bug_matches'])])
        indexing_bugs = len(
            [c for c in bug_candidates if any('indexing' in match or 'subgraph' in match or 'query' in match
                                              for match in c['thegraph_keyword_matches'] + c['thegraph_bug_matches'])])

        print(f"   - Stakingç›¸å…³bug: {staking_bugs}")
        print(f"   - Curationç›¸å…³bug: {curation_bugs}")
        print(f"   - Indexingç›¸å…³bug: {indexing_bugs}")

        return bug_candidates

    def export_results(self, merged_prs, bug_candidates, stats):
        """å¯¼å‡ºç»“æœåˆ°Excel"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿ç›®å½•åˆ›å»º
        excel_dir = os.path.abspath(THEGRAPH_CONFIG['excel_output'])
        os.makedirs(excel_dir, exist_ok=True)

        excel_file = os.path.join(excel_dir, f"thegraph_contracts_{timestamp}.xlsx")

        print(f"ğŸ“‚ æ­£åœ¨åˆ›å»ºExcelæ–‡ä»¶...")
        print(f"   ç›®å½•: {excel_dir}")
        print(f"   æ–‡ä»¶: thegraph_contracts_{timestamp}.xlsx")

        try:
            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                # 1. æ‰€æœ‰åˆå¹¶çš„PR
                merged_df = pd.DataFrame(merged_prs)
                merged_df.to_excel(writer, sheet_name='All_Merged_PRs', index=False)

                # 2. ç–‘ä¼¼bugä¿®å¤PR
                if bug_candidates:
                    bug_df = pd.DataFrame(bug_candidates)
                    # é€‰æ‹©é‡è¦åˆ—
                    bug_display_df = bug_df[[
                        'number', 'title', 'user', 'merged_at', 'match_score', 'confidence',
                        'general_keyword_matches', 'thegraph_keyword_matches', 'label_matches',
                        'project_name', 'project_type', 'project_domain', 'url'
                    ]].copy()

                    # æ ¼å¼åŒ–åŒ¹é…ç»“æœ
                    bug_display_df['general_keyword_matches'] = bug_display_df['general_keyword_matches'].apply(
                        lambda x: ', '.join(x[:5]))
                    bug_display_df['thegraph_keyword_matches'] = bug_display_df['thegraph_keyword_matches'].apply(
                        lambda x: ', '.join(x[:5]))
                    bug_display_df['label_matches'] = bug_display_df['label_matches'].apply(lambda x: ', '.join(x))

                    bug_display_df.to_excel(writer, sheet_name='Bug_Fix_Candidates', index=False)

                # 3. ç»Ÿè®¡ä¿¡æ¯
                stats_data = [
                    ['é¡¹ç›®åç§°', 'The Graph'],
                    ['é¡¹ç›®ç±»å‹', 'Infrastructure'],
                    ['é¡¹ç›®é¢†åŸŸ', 'Indexing Protocol'],
                    ['ä»“åº“åœ°å€', f"{self.owner}/{self.repo}"],
                    ['æ€»åˆå¹¶PRæ•°', stats['total_prs']],
                    ['Stakingç›¸å…³PRæ•°', stats['staking_prs']],
                    ['Curationç›¸å…³PRæ•°', stats['curation_prs']],
                    ['Indexingç›¸å…³PRæ•°', stats['indexing_prs']],
                    ['ç–‘ä¼¼bugä¿®å¤PRæ•°', len(bug_candidates)],
                    ['æœ€æ´»è·ƒè´¡çŒ®è€…', stats['user_counts'].index[0] if not stats['user_counts'].empty else 'N/A'],
                    ['æ€»ä»£ç å¢åŠ è¡Œæ•°', stats['code_stats']['additions']],
                    ['æ€»ä»£ç åˆ é™¤è¡Œæ•°', stats['code_stats']['deletions']],
                    ['æ€»å˜æ›´æ–‡ä»¶æ•°', stats['code_stats']['files']]
                ]

                stats_df = pd.DataFrame(stats_data, columns=['æŒ‡æ ‡', 'æ•°å€¼'])
                stats_df.to_excel(writer, sheet_name='Statistics', index=False)

                # 4. æ—¶é—´è¶‹åŠ¿
                time_df = stats['date_counts'].reset_index()
                time_df.columns = ['æ—¥æœŸ', 'PRæ•°é‡']
                time_df.to_excel(writer, sheet_name='Time_Trends', index=False)

                # 5. ç½®ä¿¡åº¦åˆ†å¸ƒ
                if bug_candidates:
                    confidence_counts = pd.Series([c['confidence'] for c in bug_candidates]).value_counts()
                    confidence_df = confidence_counts.reset_index()
                    confidence_df.columns = ['ç½®ä¿¡åº¦', 'æ•°é‡']
                    confidence_df.to_excel(writer, sheet_name='Confidence_Distribution', index=False)

                # 6. The GraphåŠŸèƒ½åˆ†ç±»
                if bug_candidates:
                    function_data = []
                    for candidate in bug_candidates:
                        functions = []
                        matches = candidate['thegraph_keyword_matches'] + candidate['thegraph_bug_matches']

                        if any('staking' in match or 'delegation' in match or 'slash' in match for match in matches):
                            functions.append('Staking')
                        if any('curation' in match or 'signal' in match or 'curator' in match for match in matches):
                            functions.append('Curation')
                        if any('indexing' in match or 'subgraph' in match or 'query' in match for match in matches):
                            functions.append('Indexing')

                        function_data.append({
                            'PR_Number': candidate['number'],
                            'Title': candidate['title'],
                            'Functions': ', '.join(functions) if functions else 'General',
                            'Confidence': candidate['confidence']
                        })

                    function_df = pd.DataFrame(function_data)
                    function_df.to_excel(writer, sheet_name='Function_Classification', index=False)

            # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„åˆ›å»ºæˆåŠŸ
            if os.path.exists(excel_file):
                file_size = os.path.getsize(excel_file)
                print(f"âœ… æ–‡ä»¶åˆ›å»ºæˆåŠŸï¼")
                print(f"   å¤§å°: {file_size:,} bytes")
            else:
                print(f"âŒ æ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼")

        except Exception as e:
            print(f"âŒ å¯¼å‡ºExcelæ—¶å‡ºé”™: {e}")
            excel_file = None

        print(f"ğŸ“ The Graph Contractsç»“æœå·²å¯¼å‡ºåˆ°: {excel_file}")
        print(f"ğŸ“‚ å®Œæ•´è·¯å¾„: {os.path.abspath(excel_file) if excel_file else 'N/A'}")
        return excel_file

    def make_request(self, url, params=None):
        """å‘é€APIè¯·æ±‚"""
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 403:
                print("âš ï¸  APIé…é¢å¯èƒ½ä¸è¶³ï¼Œè¯·ç¨åé‡è¯•")
                return None
            else:
                print(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
        except Exception as e:
            print(f"è¯·æ±‚å¼‚å¸¸: {e}")
            return None

    def run_collection(self):
        """è¿è¡Œå®Œæ•´çš„æ”¶é›†æµç¨‹"""
        print("ğŸš€ å¼€å§‹æ”¶é›†The Graph Contractså·²åˆå¹¶çš„PR...")
        print("ğŸ“– å®éªŒæµç¨‹ï¼šä¸“é—¨åˆ†æSolidityæ™ºèƒ½åˆçº¦ä»“åº“")
        print("ğŸ”— é¡¹ç›®ï¼šThe Graph - å»ä¸­å¿ƒåŒ–ç´¢å¼•åè®®")
        print(f"ğŸ“ ä»“åº“ï¼š{self.owner}/{self.repo}")

        # 1. æ”¶é›†æ‰€æœ‰å·²åˆå¹¶çš„PR
        merged_prs = self.collect_all_merged_prs()

        if not merged_prs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å·²åˆå¹¶çš„PR")
            return

        # 2. åˆ†æPRæ•°æ®
        stats = self.analyze_merged_prs(merged_prs)

        # 3. è¯†åˆ«bugä¿®å¤ç›¸å…³çš„PR
        bug_candidates = self.identify_bug_fix_prs(merged_prs)

        # 4. å¯¼å‡ºç»“æœ
        excel_file = self.export_results(merged_prs, bug_candidates, stats)

        print(f"\nâœ… The Graph Contractsæ•°æ®æ”¶é›†å®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœæ‘˜è¦:")
        print(f"   - é¡¹ç›®: The Graph")
        print(f"   - ç±»å‹: Infrastructure")
        print(f"   - é¢†åŸŸ: Indexing Protocol")
        print(f"   - æ€»åˆå¹¶PR: {len(merged_prs)}")
        print(f"   - StakingåŠŸèƒ½PR: {stats['staking_prs']}")
        print(f"   - CurationåŠŸèƒ½PR: {stats['curation_prs']}")
        print(f"   - IndexingåŠŸèƒ½PR: {stats['indexing_prs']}")
        print(f"   - ç–‘ä¼¼bugä¿®å¤: {len(bug_candidates)}")
        print(f"   - ç»“æœæ–‡ä»¶: {excel_file}")

        # æ˜¾ç¤ºé¡¹ç›®ç›®å½•ç»“æ„
        print(f"\nğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•ç»“æ„:")
        output_base = os.path.abspath('./output')
        if os.path.exists(output_base):
            for root, dirs, files in os.walk(output_base):
                level = root.replace(output_base, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f"{indent}{os.path.basename(root)}/")
                subindent = ' ' * 2 * (level + 1)
                for file in files:
                    print(f"{subindent}{file}")

        print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥:")
        print(f"   1. äººå·¥å®¡æ ¸ç–‘ä¼¼bugä¿®å¤PRåˆ—è¡¨")
        print(f"   2. ç¡®è®¤çœŸæ­£çš„bugä¿®å¤å®ä¾‹")
        print(f"   3. æŒ‰8ç§bugç±»å‹è¿›è¡Œåˆ†ç±»")
        print(f"   4. åˆ†æInfrastructure vs DeFiçš„å·®å¼‚")


if __name__ == "__main__":
    collector = TheGraphContractsCollector()
    collector.run_collection()