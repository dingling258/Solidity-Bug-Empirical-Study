import sys
import os

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import requests
import pandas as pd
import json
import re
from datetime import datetime
from config.settings_template import GITHUB_TOKEN, MAKERDAO_CONFIG


class MakerDAODSSCollector:
    def __init__(self):
        self.headers = {'Authorization': f'token {GITHUB_TOKEN}'}
        self.owner = MAKERDAO_CONFIG['owner']
        self.repo = MAKERDAO_CONFIG['repo']
        self.base_url = f"https://api.github.com/repos/{self.owner}/{self.repo}"

        # é€šç”¨bugç›¸å…³å…³é”®è¯ï¼ˆä¸åŸç ”ç©¶ä¿æŒä¸€è‡´ï¼‰
        self.general_bug_keywords = [
            'bug', 'fix', 'repair', 'defect', 'vulnerability', 'issue',
            'error', 'problem', 'incorrect', 'wrong', 'fail', 'crash',
            'security', 'exploit', 'attack', 'overflow', 'underflow',
            'reentrancy', 'gas', 'optimization', 'revert', 'panic'
        ]

        # MakerDAO DSSç‰¹å®šå…³é”®è¯
        self.makerdao_keywords = [
            'dai', 'cdp', 'vault', 'collateral', 'stability', 'maker', 'governance',
            'liquidation', 'auction', 'flapper', 'flopper', 'vow', 'jug', 'spot',
            'end', 'pause', 'proxy', 'registry', 'chief', 'hat', 'spell',
            'debt', 'surplus', 'deficit', 'rate', 'fee', 'penalty', 'kick',
            'bite', 'bark', 'dent', 'deal', 'tend', 'clip', 'calc', 'dog',
            'gem', 'join', 'adapt', 'flip', 'flop', 'hope', 'nope', 'rely',
            'deny', 'file', 'drip', 'fold', 'grab', 'heal', 'kiss', 'suck',
            'bump', 'dump', 'yank', 'cage', 'free', 'pack', 'cash', 'exit',
            'quit', 'move', 'flux', 'fork', 'frob', 'slip', 'toll', 'chop',
            'lump', 'step', 'cut', 'cusp', 'chip', 'tip', 'chost', 'buf',
            'tail', 'tau', 'ttl', 'lot', 'bid', 'guy', 'tic', 'gal', 'tab',
            'rad', 'wad', 'ray', 'pot', 'pie', 'chi', 'rho', 'dsr', 'live',
            'vat', 'cat', 'box', 'ilk', 'urn', 'gem', 'ink', 'art', 'dust',
            'line', 'hole', 'dirt', 'room', 'sin', 'vice', 'ash', 'on',
            'mkr', 'gov', 'iou', 'lock', 'free', 'vote', 'lift', 'slate'
        ]

        # åˆå¹¶æ‰€æœ‰å…³é”®è¯
        self.bug_keywords = self.general_bug_keywords + self.makerdao_keywords

        self.merged_prs = []

    def collect_all_merged_prs(self):
        """æ”¶é›†æ‰€æœ‰å·²åˆå¹¶çš„PR"""
        print("ğŸ“¥ æ­£åœ¨æ”¶é›†MakerDAO DSSæ‰€æœ‰å·²åˆå¹¶çš„PR...")
        print(f"ğŸ”— ä»“åº“: {self.owner}/{self.repo}")

        merged_prs = []
        page = 1
        total_collected = 0

        while True:
            print(f"   æ­£åœ¨è·å–ç¬¬ {page} é¡µ...")

            # åªè·å–mergedçŠ¶æ€çš„PR
            prs = self.make_request(f"{self.base_url}/pulls", {
                'state': 'closed',  # GitHub API: closedåŒ…å«mergedå’Œæœªmergedçš„
                'per_page': 100,
                'page': page,
                'sort': 'updated',
                'direction': 'desc'
            })

            if not prs:
                break

            # ç­›é€‰å‡ºçœŸæ­£mergedçš„PR
            page_merged_count = 0
            for pr in prs:
                if pr.get('merged_at') is not None:  # å…³é”®ï¼šåªè¦merged_atä¸ä¸ºç©º
                    merged_prs.append({
                        'project_name': 'MakerDAO',
                        'project_type': 'DeFi',
                        'project_domain': 'Stablecoin Protocol',
                        'number': pr['number'],
                        'title': pr['title'],
                        'body': pr.get('body', '') or '',
                        'state': pr['state'],
                        'merged_at': pr['merged_at'],
                        'created_at': pr['created_at'],
                        'user': pr['user']['login'],
                        'url': pr['html_url'],
                        'labels': [label['name'] for label in pr.get('labels', [])],
                        'commits': pr.get('commits', 0),
                        'additions': pr.get('additions', 0),
                        'deletions': pr.get('deletions', 0),
                        'changed_files': pr.get('changed_files', 0),
                        'assignees': [assignee['login'] for assignee in pr.get('assignees', [])],
                        'milestone': pr.get('milestone', {}).get('title', '') if pr.get('milestone') else '',
                        'base_ref': pr.get('base', {}).get('ref', ''),
                        'head_ref': pr.get('head', {}).get('ref', '')
                    })
                    page_merged_count += 1

            total_collected += page_merged_count
            print(f"   ç¬¬ {page} é¡µæ‰¾åˆ° {page_merged_count} ä¸ªåˆå¹¶çš„PR (æ€»è®¡: {total_collected})")

            # å¦‚æœè¿™ä¸€é¡µæ²¡æœ‰mergedçš„PRï¼Œå¯èƒ½å·²ç»åˆ°åº•äº†
            if page_merged_count == 0:
                break

            page += 1

        print(f"âœ… æ€»å…±æ”¶é›†åˆ° {len(merged_prs)} ä¸ªå·²åˆå¹¶çš„PR")
        return merged_prs

    def analyze_merged_prs(self, merged_prs):
        """åˆ†æå·²åˆå¹¶çš„PR"""
        print("ğŸ“Š åˆ†æMakerDAO DSSå·²åˆå¹¶çš„PR...")

        # åŸºæœ¬ç»Ÿè®¡
        total_prs = len(merged_prs)

        # æ—¶é—´åˆ†æ
        dates = [pr['merged_at'][:10] for pr in merged_prs]
        date_counts = pd.Series(dates).value_counts().sort_index()

        # ç”¨æˆ·åˆ†æ
        users = [pr['user'] for pr in merged_prs]
        user_counts = pd.Series(users).value_counts()

        # æ ‡ç­¾åˆ†æ
        all_labels = []
        for pr in merged_prs:
            all_labels.extend(pr['labels'])
        label_counts = pd.Series(all_labels).value_counts()

        # ä»£ç å˜æ›´åˆ†æ
        total_additions = sum(pr['additions'] for pr in merged_prs)
        total_deletions = sum(pr['deletions'] for pr in merged_prs)
        total_files = sum(pr['changed_files'] for pr in merged_prs)

        # MakerDAOç‰¹å®šåˆ†æ
        vault_keywords = ['vault', 'cdp', 'collateral', 'liquidation', 'auction']
        governance_keywords = ['governance', 'vote', 'spell', 'chief', 'hat', 'mkr']
        stability_keywords = ['dai', 'stability', 'rate', 'fee', 'dsr', 'surplus']

        vault_prs = [pr for pr in merged_prs
                     if any(keyword in pr['title'].lower() or keyword in pr['body'].lower()
                            for keyword in vault_keywords)]

        governance_prs = [pr for pr in merged_prs
                         if any(keyword in pr['title'].lower() or keyword in pr['body'].lower()
                                for keyword in governance_keywords)]

        stability_prs = [pr for pr in merged_prs
                        if any(keyword in pr['title'].lower() or keyword in pr['body'].lower()
                               for keyword in stability_keywords)]

        print(f"ğŸ“ˆ MakerDAO DSSç»Ÿè®¡ç»“æœ:")
        print(f"   - æ€»åˆå¹¶PRæ•°: {total_prs}")
        print(f"   - Vaultç›¸å…³PRæ•°: {len(vault_prs)}")
        print(f"   - Governanceç›¸å…³PRæ•°: {len(governance_prs)}")
        print(f"   - Stabilityç›¸å…³PRæ•°: {len(stability_prs)}")
        print(f"   - æœ€æ—©åˆå¹¶æ—¥æœŸ: {min(dates) if dates else 'N/A'}")
        print(f"   - æœ€æ™šåˆå¹¶æ—¥æœŸ: {max(dates) if dates else 'N/A'}")
        print(
            f"   - æœ€æ´»è·ƒè´¡çŒ®è€…: {user_counts.head(1).index[0] if not user_counts.empty else 'N/A'} ({user_counts.iloc[0] if not user_counts.empty else 0} PRs)")
        print(f"   - æ€»ä»£ç è¡Œå˜æ›´: +{total_additions:,} -{total_deletions:,}")
        print(f"   - æ€»æ–‡ä»¶å˜æ›´: {total_files:,}")

        return {
            'total_prs': total_prs,
            'vault_prs': len(vault_prs),
            'governance_prs': len(governance_prs),
            'stability_prs': len(stability_prs),
            'date_counts': date_counts,
            'user_counts': user_counts,
            'label_counts': label_counts,
            'code_stats': {
                'additions': total_additions,
                'deletions': total_deletions,
                'files': total_files
            }
        }

    def identify_bug_fix_prs(self, merged_prs):
        """ä»å·²åˆå¹¶çš„PRä¸­è¯†åˆ«bugä¿®å¤ç›¸å…³çš„PRï¼ˆéµå¾ªåŸç ”ç©¶æ–¹æ³•è®ºï¼‰"""
        print("ğŸ” è¯†åˆ«MakerDAO DSS bugä¿®å¤ç›¸å…³çš„PR...")

        bug_candidates = []

        for pr in merged_prs:
            title_lower = pr['title'].lower()
            body_lower = pr['body'].lower()
            labels_lower = [label.lower() for label in pr['labels']]

            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            title_body_text = title_lower + ' ' + body_lower

            # é€šç”¨bugå…³é”®è¯åŒ¹é…
            general_keyword_matches = [kw for kw in self.general_bug_keywords if kw in title_body_text]

            # MakerDAOç‰¹å®šå…³é”®è¯åŒ¹é…
            makerdao_keyword_matches = [kw for kw in self.makerdao_keywords if kw in title_body_text]

            # æ£€æŸ¥æ ‡ç­¾
            bug_labels = ['bug', 'defect', 'security', 'vulnerability', 'fix', 'hotfix', 'patch', 'critical']
            label_matches = [label for label in labels_lower if any(bug_label in label for bug_label in bug_labels)]

            # æ£€æŸ¥fixå¼•ç”¨æ¨¡å¼
            fix_patterns = [
                r'fix(?:es)?\s*#?\d+',  # fixes #123
                r'resolv(?:es)?\s*#?\d+',  # resolves #123
                r'clos(?:es)?\s*#?\d+',  # closes #123
                r'fix(?:es)?\s+\w+',  # fixes bug
                r'patch(?:es)?\s+\w+',  # patches issue
            ]
            fix_references = []
            for pattern in fix_patterns:
                fix_references.extend(re.findall(pattern, title_body_text))

            # MakerDAOç‰¹å®šçš„bugæ¨¡å¼
            makerdao_bug_patterns = [
                r'vault.*(?:fail|error|bug|incorrect|liquidat)',
                r'auction.*(?:fail|error|bug|wrong|invalid)',
                r'governance.*(?:fail|error|bug|vote|spell)',
                r'dai.*(?:fail|error|bug|peg|stability|rate)',
                r'collateral.*(?:fail|error|bug|lock|unlock)',
                r'liquidation.*(?:fail|error|bug|penalty|auction)',
                r'surplus.*(?:fail|error|bug|auction|buffer)',
                r'deficit.*(?:fail|error|bug|auction|debt)',
                r'oracle.*(?:fail|error|bug|price|feed)',
                r'flash.*(?:fail|error|bug|loan|attack)',
                r'proxy.*(?:fail|error|bug|delegate|call)',
                r'pause.*(?:fail|error|bug|guardian|delay)',
                r'spell.*(?:fail|error|bug|cast|schedule)',
                r'chief.*(?:fail|error|bug|vote|hat)',
                r'pot.*(?:fail|error|bug|drip|dsr)',
                r'vat.*(?:fail|error|bug|frob|grab)',
                r'cat.*(?:fail|error|bug|bite|flip)',
                r'dog.*(?:fail|error|bug|bark|clip)',
                r'jug.*(?:fail|error|bug|drip|base)',
                r'spot.*(?:fail|error|bug|poke|par)'
            ]

            makerdao_bug_matches = []
            for pattern in makerdao_bug_patterns:
                makerdao_bug_matches.extend(re.findall(pattern, title_body_text))

            # è®¡ç®—åŒ¹é…åˆ†æ•°ï¼ˆä¸åŸç ”ç©¶æ–¹æ³•è®ºä¸€è‡´ï¼‰
            match_score = (len(general_keyword_matches) +
                           len(label_matches) +
                           len(fix_references) +
                           len(makerdao_bug_matches))

            if general_keyword_matches or label_matches or fix_references or makerdao_bug_matches:
                confidence = 'high' if match_score >= 3 else 'medium' if match_score >= 1 else 'low'

                bug_candidates.append({
                    **pr,
                    'general_keyword_matches': general_keyword_matches,
                    'makerdao_keyword_matches': makerdao_keyword_matches,
                    'label_matches': label_matches,
                    'fix_references': fix_references,
                    'makerdao_bug_matches': makerdao_bug_matches,
                    'match_score': match_score,
                    'confidence': confidence
                })

        print(f"âœ… ä» {len(merged_prs)} ä¸ªåˆå¹¶PRä¸­è¯†åˆ«å‡º {len(bug_candidates)} ä¸ªç–‘ä¼¼bugä¿®å¤PR")

        # æŒ‰ç½®ä¿¡åº¦åˆ†ç±»ç»Ÿè®¡
        high_confidence = len([c for c in bug_candidates if c['confidence'] == 'high'])
        medium_confidence = len([c for c in bug_candidates if c['confidence'] == 'medium'])
        low_confidence = len([c for c in bug_candidates if c['confidence'] == 'low'])

        print(f"   - é«˜ç½®ä¿¡åº¦: {high_confidence}")
        print(f"   - ä¸­ç½®ä¿¡åº¦: {medium_confidence}")
        print(f"   - ä½ç½®ä¿¡åº¦: {low_confidence}")

        # æŒ‰MakerDAOåŠŸèƒ½åˆ†ç±»ç»Ÿè®¡
        vault_bugs = len(
            [c for c in bug_candidates if any('vault' in match or 'cdp' in match or 'liquidat' in match
                                              for match in c['makerdao_keyword_matches'] + c['makerdao_bug_matches'])])
        governance_bugs = len(
            [c for c in bug_candidates if any('governance' in match or 'vote' in match or 'spell' in match
                                              for match in c['makerdao_keyword_matches'] + c['makerdao_bug_matches'])])
        stability_bugs = len(
            [c for c in bug_candidates if any('dai' in match or 'stability' in match or 'rate' in match
                                              for match in c['makerdao_keyword_matches'] + c['makerdao_bug_matches'])])

        print(f"   - Vaultç›¸å…³bug: {vault_bugs}")
        print(f"   - Governanceç›¸å…³bug: {governance_bugs}")
        print(f"   - Stabilityç›¸å…³bug: {stability_bugs}")

        return bug_candidates

    def export_results(self, merged_prs, bug_candidates, stats):
        """å¯¼å‡ºç»“æœåˆ°Excel"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿ç›®å½•åˆ›å»º
        excel_dir = os.path.abspath(MAKERDAO_CONFIG['excel_output'])
        os.makedirs(excel_dir, exist_ok=True)

        excel_file = os.path.join(excel_dir, f"makerdao_dss_{timestamp}.xlsx")

        print(f"ğŸ“‚ æ­£åœ¨åˆ›å»ºExcelæ–‡ä»¶...")
        print(f"   ç›®å½•: {excel_dir}")
        print(f"   æ–‡ä»¶: makerdao_dss_{timestamp}.xlsx")

        try:
            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                # 1. æ‰€æœ‰åˆå¹¶çš„PR
                merged_df = pd.DataFrame(merged_prs)
                merged_df.to_excel(writer, sheet_name='All_Merged_PRs', index=False)

                # 2. ç–‘ä¼¼bugä¿®å¤PR
                if bug_candidates:
                    bug_df = pd.DataFrame(bug_candidates)
                    # é€‰æ‹©é‡è¦åˆ—
                    bug_display_df = bug_df[[
                        'number', 'title', 'user', 'merged_at', 'match_score', 'confidence',
                        'general_keyword_matches', 'makerdao_keyword_matches', 'label_matches',
                        'project_name', 'project_type', 'project_domain', 'url'
                    ]].copy()

                    # æ ¼å¼åŒ–åŒ¹é…ç»“æœ
                    bug_display_df['general_keyword_matches'] = bug_display_df['general_keyword_matches'].apply(
                        lambda x: ', '.join(x[:5]))
                    bug_display_df['makerdao_keyword_matches'] = bug_display_df['makerdao_keyword_matches'].apply(
                        lambda x: ', '.join(x[:5]))
                    bug_display_df['label_matches'] = bug_display_df['label_matches'].apply(lambda x: ', '.join(x))

                    bug_display_df.to_excel(writer, sheet_name='Bug_Fix_Candidates', index=False)

                # 3. ç»Ÿè®¡ä¿¡æ¯
                stats_data = [
                    ['é¡¹ç›®åç§°', 'MakerDAO'],
                    ['é¡¹ç›®ç±»å‹', 'DeFi'],
                    ['é¡¹ç›®é¢†åŸŸ', 'Stablecoin Protocol'],
                    ['ä»“åº“åœ°å€', f"{self.owner}/{self.repo}"],
                    ['æ€»åˆå¹¶PRæ•°', stats['total_prs']],
                    ['Vaultç›¸å…³PRæ•°', stats['vault_prs']],
                    ['Governanceç›¸å…³PRæ•°', stats['governance_prs']],
                    ['Stabilityç›¸å…³PRæ•°', stats['stability_prs']],
                    ['ç–‘ä¼¼bugä¿®å¤PRæ•°', len(bug_candidates)],
                    ['æœ€æ´»è·ƒè´¡çŒ®è€…', stats['user_counts'].index[0] if not stats['user_counts'].empty else 'N/A'],
                    ['æ€»ä»£ç å¢åŠ è¡Œæ•°', stats['code_stats']['additions']],
                    ['æ€»ä»£ç åˆ é™¤è¡Œæ•°', stats['code_stats']['deletions']],
                    ['æ€»å˜æ›´æ–‡ä»¶æ•°', stats['code_stats']['files']]
                ]

                stats_df = pd.DataFrame(stats_data, columns=['æŒ‡æ ‡', 'æ•°å€¼'])
                stats_df.to_excel(writer, sheet_name='Statistics', index=False)

                # 4. æ—¶é—´è¶‹åŠ¿
                time_df = stats['date_counts'].reset_index()
                time_df.columns = ['æ—¥æœŸ', 'PRæ•°é‡']
                time_df.to_excel(writer, sheet_name='Time_Trends', index=False)

                # 5. ç½®ä¿¡åº¦åˆ†å¸ƒ
                if bug_candidates:
                    confidence_counts = pd.Series([c['confidence'] for c in bug_candidates]).value_counts()
                    confidence_df = confidence_counts.reset_index()
                    confidence_df.columns = ['ç½®ä¿¡åº¦', 'æ•°é‡']
                    confidence_df.to_excel(writer, sheet_name='Confidence_Distribution', index=False)

                # 6. MakerDAOåŠŸèƒ½åˆ†ç±»
                if bug_candidates:
                    function_data = []
                    for candidate in bug_candidates:
                        functions = []
                        matches = candidate['makerdao_keyword_matches'] + candidate['makerdao_bug_matches']

                        if any('vault' in match or 'cdp' in match or 'liquidat' in match for match in matches):
                            functions.append('Vault')
                        if any('governance' in match or 'vote' in match or 'spell' in match for match in matches):
                            functions.append('Governance')
                        if any('dai' in match or 'stability' in match or 'rate' in match for match in matches):
                            functions.append('Stability')

                        function_data.append({
                            'PR_Number': candidate['number'],
                            'Title': candidate['title'],
                            'Functions': ', '.join(functions) if functions else 'General',
                            'Confidence': candidate['confidence']
                        })

                    function_df = pd.DataFrame(function_data)
                    function_df.to_excel(writer, sheet_name='Function_Classification', index=False)

            # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„åˆ›å»ºæˆåŠŸ
            if os.path.exists(excel_file):
                file_size = os.path.getsize(excel_file)
                print(f"âœ… æ–‡ä»¶åˆ›å»ºæˆåŠŸï¼")
                print(f"   å¤§å°: {file_size:,} bytes")
            else:
                print(f"âŒ æ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼")

        except Exception as e:
            print(f"âŒ å¯¼å‡ºExcelæ—¶å‡ºé”™: {e}")
            excel_file = None

        print(f"ğŸ“ MakerDAO DSSç»“æœå·²å¯¼å‡ºåˆ°: {excel_file}")
        print(f"ğŸ“‚ å®Œæ•´è·¯å¾„: {os.path.abspath(excel_file) if excel_file else 'N/A'}")
        return excel_file

    def make_request(self, url, params=None):
        """å‘é€APIè¯·æ±‚"""
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 403:
                print("âš ï¸  APIé…é¢å¯èƒ½ä¸è¶³ï¼Œè¯·ç¨åé‡è¯•")
                return None
            else:
                print(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
        except Exception as e:
            print(f"è¯·æ±‚å¼‚å¸¸: {e}")
            return None

    def run_collection(self):
        """è¿è¡Œå®Œæ•´çš„æ”¶é›†æµç¨‹"""
        print("ğŸš€ å¼€å§‹æ”¶é›†MakerDAO DSSå·²åˆå¹¶çš„PR...")
        print("ğŸ“– å®éªŒæµç¨‹ï¼šä¸“é—¨åˆ†æSolidityæ™ºèƒ½åˆçº¦ä»“åº“")
        print("ğŸ”— é¡¹ç›®ï¼šMakerDAO - å»ä¸­å¿ƒåŒ–ç¨³å®šå¸åè®®")
        print(f"ğŸ“ ä»“åº“ï¼š{self.owner}/{self.repo}")

        # 1. æ”¶é›†æ‰€æœ‰å·²åˆå¹¶çš„PR
        merged_prs = self.collect_all_merged_prs()

        if not merged_prs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å·²åˆå¹¶çš„PR")
            return

        # 2. åˆ†æPRæ•°æ®
        stats = self.analyze_merged_prs(merged_prs)

        # 3. è¯†åˆ«bugä¿®å¤ç›¸å…³çš„PR
        bug_candidates = self.identify_bug_fix_prs(merged_prs)

        # 4. å¯¼å‡ºç»“æœ
        excel_file = self.export_results(merged_prs, bug_candidates, stats)

        print(f"\nâœ… MakerDAO DSSæ•°æ®æ”¶é›†å®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœæ‘˜è¦:")
        print(f"   - é¡¹ç›®: MakerDAO")
        print(f"   - ç±»å‹: DeFi")
        print(f"   - é¢†åŸŸ: Stablecoin Protocol")
        print(f"   - æ€»åˆå¹¶PR: {len(merged_prs)}")
        print(f"   - VaultåŠŸèƒ½PR: {stats['vault_prs']}")
        print(f"   - GovernanceåŠŸèƒ½PR: {stats['governance_prs']}")
        print(f"   - StabilityåŠŸèƒ½PR: {stats['stability_prs']}")
        print(f"   - ç–‘ä¼¼bugä¿®å¤: {len(bug_candidates)}")
        print(f"   - ç»“æœæ–‡ä»¶: {excel_file}")

        # æ˜¾ç¤ºé¡¹ç›®ç›®å½•ç»“æ„
        print(f"\nğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•ç»“æ„:")
        output_base = os.path.abspath('./output')
        if os.path.exists(output_base):
            for root, dirs, files in os.walk(output_base):
                level = root.replace(output_base, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f"{indent}{os.path.basename(root)}/")
                subindent = ' ' * 2 * (level + 1)
                for file in files:
                    print(f"{subindent}{file}")

        print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥:")
        print(f"   1. äººå·¥å®¡æ ¸ç–‘ä¼¼bugä¿®å¤PRåˆ—è¡¨")
        print(f"   2. ç¡®è®¤çœŸæ­£çš„bugä¿®å¤å®ä¾‹")
        print(f"   3. æŒ‰8ç§bugç±»å‹è¿›è¡Œåˆ†ç±»")
        print(f"   4. åˆ†æDeFiåè®®çš„ç‰¹æœ‰bugæ¨¡å¼")


if __name__ == "__main__":
    collector = MakerDAODSSCollector()
    collector.run_collection()