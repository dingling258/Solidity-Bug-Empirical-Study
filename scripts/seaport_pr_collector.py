import sys
import os
import requests
import pandas as pd
import re
from datetime import datetime

# ç¡®ä¿å¼•å…¥é…ç½®
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from config.settings_template import GITHUB_TOKEN, SEAPORT_CONFIG


class SeaportCollector:
    def __init__(self):
        self.headers = {'Authorization': f'token {GITHUB_TOKEN}'}
        self.owner = SEAPORT_CONFIG['owner']
        self.repo = SEAPORT_CONFIG['repo']
        self.base_url = f"https://api.github.com/repos/{self.owner}/{self.repo}"

        # 1. å¼ºåŠ›é»‘åå• (å™ªéŸ³è¯æ±‡) - åªè¦æ ‡é¢˜å‡ºç°è¿™äº›ï¼Œç›´æ¥æ‰”æ‰
        self.noise_keywords = [
            'typo', 'bump', 'chore', 'doc', 'docs', 'documentation',
            'lint', 'format', 'style', 'ci', 'cd', 'workflow',
            'test', 'tests', 'testing', 'coverage', 'benchmark',
            'refactor', 'rename', 'move', 'clean', 'nit',
            'release', 'version', 'merge', 'ignore', 'license'
        ]

        # 2. æ ¸å¿ƒä¿®å¤åŠ¨è¯ (å¿…é¡»å‡ºç°åœ¨æ ‡é¢˜æˆ–æ­£æ–‡å¼€å¤´)
        self.fix_verbs = [
            'fix', 'fixed', 'fixes', 'fixing',
            'resolve', 'resolved', 'resolves',
            'patch', 'patched',
            'correct', 'correction',
            'prevent', 'avoid', 'handle',  # handle edge case
            'revert', 'restore'
        ]

        # 3. Seaport æ ¸å¿ƒä¸šåŠ¡è¯æ±‡ (ç”¨äºç¡®è®¤æ˜¯ä¸šåŠ¡é€»è¾‘bugï¼Œè€Œä¸æ˜¯å·¥å…·bug)
        self.seaport_context = [
            # æ ¸å¿ƒé€»è¾‘
            'order', 'offer', 'consideration', 'fulfillment', 'match',
            'validate', 'validation', 'status', 'hash', 'eip712',
            'signature', 'digest', 'nonce', 'counter', 'zone',
            'conduit', 'transfer', 'spend', 'amount', 'recipient',
            'criteria', 'root', 'proof', 'merkle',

            # åº•å±‚/æ±‡ç¼– (Seaport ç‰¹è‰²)
            'assembly', 'yul', 'mstore', 'mload', 'sload', 'sstore',
            'memory', 'calldata', 'stack', 'overflow', 'underflow',
            'revert', 'panic', 'gas', 'limit', 'offset', 'pointer',
            'encode', 'decode', 'mask', 'bit'
        ]

    def collect_all_merged_prs(self):
        """æ”¶é›†æ‰€æœ‰å·²åˆå¹¶çš„PR"""
        print(f"ğŸ“¥ [Seaport] æ­£åœ¨æŠ“å– {self.owner}/{self.repo} ...")

        merged_prs = []
        page = 1

        while True:
            print(f"   æ­£åœ¨æ‰«æç¬¬ {page} é¡µ...")
            # GitHub API é»˜è®¤æŒ‰ created æ’åºï¼Œæˆ‘ä»¬æŒ‰ updated å€’åºï¼Œä¿è¯æ‹¿åˆ°æœ€è¿‘çš„çŠ¶æ€
            prs = self.make_request(f"{self.base_url}/pulls", {
                'state': 'closed',
                'per_page': 100,
                'page': page,
                'sort': 'updated',
                'direction': 'desc'
            })

            if not prs:
                break

            for pr in prs:
                # å¿…é¡»æ˜¯å·²åˆå¹¶çš„
                if pr.get('merged_at'):
                    merged_prs.append({
                        'number': pr['number'],
                        'title': pr['title'],
                        'body': pr.get('body', '') or '',
                        'user': pr['user']['login'],
                        'merged_at': pr['merged_at'],
                        'url': pr['html_url'],
                        'labels': [l['name'] for l in pr.get('labels', [])],
                        'additions': pr.get('additions', 0),
                        'deletions': pr.get('deletions', 0),
                        'changed_files': pr.get('changed_files', 0)
                    })

            if len(prs) < 100:
                break
            page += 1

        print(f"âœ… å…±è·å– {len(merged_prs)} ä¸ªå·²åˆå¹¶ PR")
        return merged_prs

    def is_noise(self, pr):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå™ªéŸ³PR (æµ‹è¯•ã€æ–‡æ¡£ã€ç‰ˆæœ¬æ›´æ–°ã€Typo)"""
        title_lower = pr['title'].lower()
        labels = [l.lower() for l in pr['labels']]

        # 1. æ£€æŸ¥æ ‡é¢˜ä¸­çš„é»‘åå•å…³é”®è¯
        # ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…è¯¯æ€ (ä¾‹å¦‚ 'context' åŒ…å« 'test'ï¼Œä½†ä¸åº”è¢«æ€)
        for noise in self.noise_keywords:
            # æ­£åˆ™ï¼šå•è¯è¾¹ç•Œ + å…³é”®è¯
            if re.search(r'\b' + re.escape(noise) + r'\b', title_lower):
                return True, f"Title contains '{noise}'"

        # 2. æ£€æŸ¥æ ‡ç­¾é»‘åå•
        noise_labels = ['documentation', 'dependencies', 'wontfix', 'invalid', 'question', 'duplicate']
        if any(nl in labels for nl in noise_labels):
            return True, "Label filter"

        # 3. æ£€æŸ¥Conventional Commitså‰ç¼€ (å¦‚ test: chore: docs:)
        if re.match(r'^(chore|docs|test|ci|build|style|refactor)(\(.*\))?:', title_lower):
            return True, "Conventional commit prefix"

        return False, ""

    def calculate_bug_score(self, pr):
        """
        è®¡ç®—PRæ˜¯ç¼ºé™·ä¿®å¤çš„ç½®ä¿¡åº¦åˆ†æ•°
        è¿”å›: (score, reasons_list)
        """
        score = 0
        reasons = []
        title_lower = pr['title'].lower()
        body_lower = pr['body'].lower()

        # --- è§„åˆ™ 1: æ ‡é¢˜åŒ…å«å¼ºåŠ›ä¿®å¤åŠ¨è¯ (æƒé‡æœ€é«˜) ---
        # åŒ¹é… "fix bug", "fixes issue", "fix validation" ç­‰
        for verb in self.fix_verbs:
            if re.search(r'\b' + verb + r'\b', title_lower):
                score += 10
                reasons.append(f"Title verb: {verb}")
                break  # å‘½ä¸­ä¸€ä¸ªåŠ¨è¯å³å¯

        # --- è§„åˆ™ 2: æ ‡é¢˜åŒ…å« Seaport æ ¸å¿ƒä¸Šä¸‹æ–‡ (ç¡®ä¿æ˜¯ä¸šåŠ¡é€»è¾‘) ---
        context_hits = []
        for ctx in self.seaport_context:
            if ctx in title_lower:
                context_hits.append(ctx)

        if context_hits:
            # å¦‚æœæ—¢æœ‰ fix åŠ¨è¯ï¼Œåˆæœ‰ä¸Šä¸‹æ–‡ï¼Œåˆ†æ•°æš´æ¶¨
            if score >= 10:
                score += 5 * len(context_hits)
                reasons.append(f"Context: {','.join(context_hits)}")
            else:
                # åªæœ‰ä¸Šä¸‹æ–‡æ²¡æœ‰ fix åŠ¨è¯ï¼Œå¯èƒ½æ˜¯åŠŸèƒ½æ·»åŠ ï¼Œåˆ†æ•°åŠ å¾—å°‘
                score += 1

        # --- è§„åˆ™ 3: æ ‡ç­¾ç­›é€‰ ---
        bug_labels = ['bug', 'security', 'exploit', 'vulnerability', 'high risk', 'critical']
        for label in pr['labels']:
            if any(bl in label.lower() for bl in bug_labels):
                score += 15
                reasons.append(f"Label: {label}")

        # --- è§„åˆ™ 4: æ­£æ–‡å¼•ç”¨ Issue (Fixes #123) ---
        # è¿™ç§é€šå¸¸æ˜¯çœŸæ­£çš„ä¿®å¤
        if re.search(r'(fix|close|resolve)(e?s)?\s+#\d+', body_lower) or re.search(
                r'(fix|close|resolve)(e?s)?\s+https://github.com', body_lower):
            score += 5
            reasons.append("References Issue")

        # --- è§„åˆ™ 5: çº¯æ±‡ç¼–/Gasä¼˜åŒ–ä¿®å¤ç‰¹åˆ¤ ---
        # Seaport å¾ˆå¤š bug æ˜¯ "Fix memory expansion" è¿™ç§
        if 'gas' in title_lower and ('fix' in title_lower or 'correct' in title_lower or 'leak' in title_lower):
            score += 8
            reasons.append("Gas/Assembly Fix")

        return score, reasons

    def filter_and_analyze(self, merged_prs):
        """æ‰§è¡Œç­›é€‰å’Œåˆ†æ"""
        print("ğŸ” æ­£åœ¨è¿›è¡Œæ·±åº¦ç­›é€‰ (å‰”é™¤ Typo/Test/Chore)...")

        candidates = []
        skipped_count = 0

        for pr in merged_prs:
            # 1. ç¬¬ä¸€è½®ï¼šå™ªéŸ³è¿‡æ»¤
            is_noise, reason = self.is_noise(pr)
            if is_noise:
                skipped_count += 1
                continue

            # 2. ç¬¬äºŒè½®ï¼šè¯„åˆ†
            score, reasons = self.calculate_bug_score(pr)

            # 3. é˜ˆå€¼æˆªæ–­
            # åªæœ‰åˆ†æ•° >= 10 çš„æ‰è¢«è®¤ä¸ºæ˜¯â€œç¼ºé™·ç›¸å…³â€
            # è¿™æ„å‘³ç€å¿…é¡»åœ¨æ ‡é¢˜ä¸­æœ‰ fix åŠ¨è¯ï¼Œæˆ–è€…æœ‰ bug æ ‡ç­¾
            if score >= 10:
                confidence = 'High' if score >= 20 else 'Medium'

                # æå–å…·ä½“çš„ Bug å…³é”®è¯ç”¨äºåˆ†ç±»
                keywords = [k for k in self.seaport_context if k in pr['title'].lower()]

                candidates.append({
                    'number': pr['number'],
                    'title': pr['title'],
                    'score': score,
                    'confidence': confidence,
                    'reasons': ", ".join(reasons),
                    'keywords': ", ".join(keywords[:5]),
                    'url': pr['url'],
                    'merged_at': pr['merged_at'],
                    'body': pr['body'][:200].replace('\n', ' ') + '...'  # æˆªå–éƒ¨åˆ†æ­£æ–‡
                })

        # æŒ‰åˆ†æ•°å€’åºæ’åˆ—ï¼Œåˆ†æ•°é«˜çš„åœ¨æœ€å‰é¢
        candidates.sort(key=lambda x: x['score'], reverse=True)

        print(f"ğŸ“‰ è¿‡æ»¤ç»Ÿè®¡:")
        print(f"   - åŸå§‹ PR æ•°: {len(merged_prs)}")
        print(f"   - å™ªéŸ³å‰”é™¤æ•°: {skipped_count} (Typo, Tests, Docs, Bumps)")
        print(f"   - ç–‘ä¼¼ç¼ºé™·æ•°: {len(candidates)}")

        return candidates

    def export_to_excel(self, candidates):
        if not candidates:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç¼ºé™·ä¿®å¤ PR")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(SEAPORT_CONFIG['excel_output'], f"seaport_strict_bugs_{timestamp}.xlsx")

        df = pd.DataFrame(candidates)

        # é‡æ–°æ’åºåˆ—ï¼ŒæŠŠé‡è¦çš„æ”¾åœ¨å‰é¢
        cols = ['number', 'score', 'confidence', 'title', 'keywords', 'reasons', 'url', 'merged_at']
        df = df[cols]

        try:
            df.to_excel(output_path, index=False)
            print(f"âœ… ç»“æœå·²å¯¼å‡º: {output_path}")
            print("   (è¯·æ‰“å¼€ Excel æŸ¥çœ‹ Score è¾ƒé«˜çš„æ¡ç›®ï¼ŒTop 20 åº”è¯¥æ˜¯çœŸæ­£çš„ä»£ç é€»è¾‘ä¿®å¤)")
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")

    def make_request(self, url, params=None):
        try:
            resp = requests.get(url, headers=self.headers, params=params, timeout=20)
            if resp.status_code == 200:
                return resp.json()
            elif resp.status_code == 403:
                print("âš ï¸ API Rate Limit Exceeded")
            return []
        except Exception as e:
            print(f"Error: {e}")
            return []

    def run(self):
        # 1. è·å–
        all_prs = self.collect_all_merged_prs()
        # 2. ä¸¥æ ¼ç­›é€‰
        bug_prs = self.filter_and_analyze(all_prs)
        # 3. å¯¼å‡º
        self.export_to_excel(bug_prs)


if __name__ == "__main__":
    collector = SeaportCollector()
    collector.run()