import sys
import os

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import requests
import pandas as pd
import json
import re
from datetime import datetime
from config.settings_template import GITHUB_TOKEN, ROCKETPOOL_CONFIG


class RocketPoolCollector:
    def __init__(self):
        self.headers = {'Authorization': f'token {GITHUB_TOKEN}'}
        self.owner = ROCKETPOOL_CONFIG['owner']
        self.repo = ROCKETPOOL_CONFIG['repo']
        self.base_url = f"https://api.github.com/repos/{self.owner}/{self.repo}"

        # é€šç”¨bugç›¸å…³å…³é”®è¯ï¼ˆä¸åŸç ”ç©¶ä¿æŒä¸€è‡´ï¼‰
        self.general_bug_keywords = [
            'bug', 'fix', 'repair', 'defect', 'vulnerability', 'issue',
            'error', 'problem', 'incorrect', 'wrong', 'fail', 'crash',
            'security', 'exploit', 'attack', 'overflow', 'underflow',
            'reentrancy', 'gas', 'optimization', 'revert', 'panic'
        ]

        # Rocket Poolç‰¹å®šå…³é”®è¯
        self.rocketpool_keywords = [
            # æ ¸å¿ƒè´¨æŠ¼æ¦‚å¿µ
            'staking', 'stake', 'staker', 'validator', 'node', 'operator',
            'minipool', 'deposit', 'withdrawal', 'unstaking', 'unstake',

            # ä»£å¸ç›¸å…³
            'reth', 'rpl', 'eth', 'reward', 'commission', 'penalty',
            'slash', 'slashing', 'balance', 'supply', 'mint', 'burn',

            # ç½‘ç»œå’ŒéªŒè¯å™¨
            'beacon', 'consensus', 'execution', 'layer', 'client',
            'attestation', 'proposal', 'epoch', 'slot', 'sync',

            # Rocket Poolç‰¹æœ‰æ¨¡å—
            'rocket', 'pool', 'minipool', 'smoothing', 'auction',
            'storage', 'vault', 'treasury', 'claim', 'merkle',

            # æ²»ç†å’ŒDAO
            'dao', 'governance', 'proposal', 'vote', 'snapshot',
            'oracle', 'trusted', 'guardian', 'protocol', 'settings',

            # è´¨æŠ¼æµç¨‹
            'queue', 'assigned', 'staking', 'stakeable', 'prelaunch',
            'initialized', 'dissolved', 'finalised', 'distributable',

            # å¥–åŠ±å’Œæƒ©ç½š
            'smoothing', 'pool', 'merkle', 'proof', 'tree', 'interval',
            'claim', 'claimable', 'distribute', 'distribution',

            # ç½‘ç»œè´¹ç”¨å’Œé…ç½®
            'network', 'price', 'ratio', 'threshold', 'timeout',
            'cooldown', 'period', 'interval', 'rate', 'fee',

            # å®‰å…¨å’Œè®¿é—®æ§åˆ¶
            'guardian', 'admin', 'role', 'permission', 'access',
            'upgrade', 'proxy', 'implementation', 'delegate',

            # å­˜å‚¨å’ŒçŠ¶æ€
            'storage', 'state', 'status', 'phase', 'stage',
            'checkpoint', 'snapshot', 'record', 'history',

            # é›†æˆå’Œæ¥å£
            'interface', 'manager', 'contract', 'registry', 'factory',
            'helper', 'utility', 'wrapper', 'adapter', 'bridge'
        ]

        # åˆå¹¶æ‰€æœ‰å…³é”®è¯
        self.bug_keywords = self.general_bug_keywords + self.rocketpool_keywords

        self.merged_prs = []

    def collect_all_merged_prs(self):
        """æ”¶é›†æ‰€æœ‰å·²åˆå¹¶çš„PR"""
        print("ğŸ“¥ æ­£åœ¨æ”¶é›†Rocket Poolæ‰€æœ‰å·²åˆå¹¶çš„PR...")
        print(f"ğŸ”— ä»“åº“: {self.owner}/{self.repo}")

        merged_prs = []
        page = 1
        total_collected = 0

        while True:
            print(f"   æ­£åœ¨è·å–ç¬¬ {page} é¡µ...")

            # åªè·å–mergedçŠ¶æ€çš„PR
            prs = self.make_request(f"{self.base_url}/pulls", {
                'state': 'closed',  # GitHub API: closedåŒ…å«mergedå’Œæœªmergedçš„
                'per_page': 100,
                'page': page,
                'sort': 'updated',
                'direction': 'desc'
            })

            if not prs:
                break

            # ç­›é€‰å‡ºçœŸæ­£mergedçš„PR
            page_merged_count = 0
            for pr in prs:
                if pr.get('merged_at') is not None:  # å…³é”®ï¼šåªè¦merged_atä¸ä¸ºç©º
                    merged_prs.append({
                        'project_name': 'RocketPool',
                        'project_type': 'DeFi',
                        'project_domain': 'Ethereum Staking Protocol',
                        'number': pr['number'],
                        'title': pr['title'],
                        'body': pr.get('body', '') or '',
                        'state': pr['state'],
                        'merged_at': pr['merged_at'],
                        'created_at': pr['created_at'],
                        'user': pr['user']['login'],
                        'url': pr['html_url'],
                        'labels': [label['name'] for label in pr.get('labels', [])],
                        'commits': pr.get('commits', 0),
                        'additions': pr.get('additions', 0),
                        'deletions': pr.get('deletions', 0),
                        'changed_files': pr.get('changed_files', 0),
                        'assignees': [assignee['login'] for assignee in pr.get('assignees', [])],
                        'milestone': pr.get('milestone', {}).get('title', '') if pr.get('milestone') else '',
                        'base_ref': pr.get('base', {}).get('ref', ''),
                        'head_ref': pr.get('head', {}).get('ref', '')
                    })
                    page_merged_count += 1

            total_collected += page_merged_count
            print(f"   ç¬¬ {page} é¡µæ‰¾åˆ° {page_merged_count} ä¸ªåˆå¹¶çš„PR (æ€»è®¡: {total_collected})")

            # å¦‚æœè¿™ä¸€é¡µæ²¡æœ‰mergedçš„PRï¼Œå¯èƒ½å·²ç»åˆ°åº•äº†
            if page_merged_count == 0:
                break

            page += 1

        print(f"âœ… æ€»å…±æ”¶é›†åˆ° {len(merged_prs)} ä¸ªå·²åˆå¹¶çš„PR")
        return merged_prs

    def analyze_merged_prs(self, merged_prs):
        """åˆ†æå·²åˆå¹¶çš„PR"""
        print("ğŸ“Š åˆ†æRocket Poolå·²åˆå¹¶çš„PR...")

        # åŸºæœ¬ç»Ÿè®¡
        total_prs = len(merged_prs)

        # æ—¶é—´åˆ†æ
        dates = [pr['merged_at'][:10] for pr in merged_prs]
        date_counts = pd.Series(dates).value_counts().sort_index()

        # ç”¨æˆ·åˆ†æ
        users = [pr['user'] for pr in merged_prs]
        user_counts = pd.Series(users).value_counts()

        # æ ‡ç­¾åˆ†æ
        all_labels = []
        for pr in merged_prs:
            all_labels.extend(pr['labels'])
        label_counts = pd.Series(all_labels).value_counts()

        # ä»£ç å˜æ›´åˆ†æ
        total_additions = sum(pr['additions'] for pr in merged_prs)
        total_deletions = sum(pr['deletions'] for pr in merged_prs)
        total_files = sum(pr['changed_files'] for pr in merged_prs)

        # Rocket Poolç‰¹å®šåˆ†æ
        staking_keywords = ['staking', 'validator', 'minipool', 'deposit', 'withdrawal', 'node']
        reward_keywords = ['reward', 'reth', 'rpl', 'commission', 'smoothing', 'claim', 'merkle']
        governance_keywords = ['dao', 'governance', 'oracle', 'guardian', 'proposal', 'vote']
        security_keywords = ['slashing', 'penalty', 'guardian', 'upgrade', 'proxy', 'access']

        staking_prs = [pr for pr in merged_prs
                       if any(keyword in pr['title'].lower() or keyword in pr['body'].lower()
                              for keyword in staking_keywords)]

        reward_prs = [pr for pr in merged_prs
                      if any(keyword in pr['title'].lower() or keyword in pr['body'].lower()
                             for keyword in reward_keywords)]

        governance_prs = [pr for pr in merged_prs
                          if any(keyword in pr['title'].lower() or keyword in pr['body'].lower()
                                 for keyword in governance_keywords)]

        security_prs = [pr for pr in merged_prs
                        if any(keyword in pr['title'].lower() or keyword in pr['body'].lower()
                               for keyword in security_keywords)]

        print(f"ğŸ“ˆ Rocket Poolç»Ÿè®¡ç»“æœ:")
        print(f"   - æ€»åˆå¹¶PRæ•°: {total_prs}")
        print(f"   - Stakingç›¸å…³PRæ•°: {len(staking_prs)}")
        print(f"   - Rewardç›¸å…³PRæ•°: {len(reward_prs)}")
        print(f"   - Governanceç›¸å…³PRæ•°: {len(governance_prs)}")
        print(f"   - Securityç›¸å…³PRæ•°: {len(security_prs)}")
        print(f"   - æœ€æ—©åˆå¹¶æ—¥æœŸ: {min(dates) if dates else 'N/A'}")
        print(f"   - æœ€æ™šåˆå¹¶æ—¥æœŸ: {max(dates) if dates else 'N/A'}")
        print(
            f"   - æœ€æ´»è·ƒè´¡çŒ®è€…: {user_counts.head(1).index[0] if not user_counts.empty else 'N/A'} ({user_counts.iloc[0] if not user_counts.empty else 0} PRs)")
        print(f"   - æ€»ä»£ç è¡Œå˜æ›´: +{total_additions:,} -{total_deletions:,}")
        print(f"   - æ€»æ–‡ä»¶å˜æ›´: {total_files:,}")

        return {
            'total_prs': total_prs,
            'staking_prs': len(staking_prs),
            'reward_prs': len(reward_prs),
            'governance_prs': len(governance_prs),
            'security_prs': len(security_prs),
            'date_counts': date_counts,
            'user_counts': user_counts,
            'label_counts': label_counts,
            'code_stats': {
                'additions': total_additions,
                'deletions': total_deletions,
                'files': total_files
            }
        }

    def identify_bug_fix_prs(self, merged_prs):
        """ä»å·²åˆå¹¶çš„PRä¸­è¯†åˆ«bugä¿®å¤ç›¸å…³çš„PRï¼ˆéµå¾ªåŸç ”ç©¶æ–¹æ³•è®ºï¼‰"""
        print("ğŸ” è¯†åˆ«Rocket Pool bugä¿®å¤ç›¸å…³çš„PR...")

        bug_candidates = []

        for pr in merged_prs:
            title_lower = pr['title'].lower()
            body_lower = pr['body'].lower()
            labels_lower = [label.lower() for label in pr['labels']]

            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            title_body_text = title_lower + ' ' + body_lower

            # é€šç”¨bugå…³é”®è¯åŒ¹é…
            general_keyword_matches = [kw for kw in self.general_bug_keywords if kw in title_body_text]

            # Rocket Poolç‰¹å®šå…³é”®è¯åŒ¹é…
            rocketpool_keyword_matches = [kw for kw in self.rocketpool_keywords if kw in title_body_text]

            # æ£€æŸ¥æ ‡ç­¾
            bug_labels = ['bug', 'defect', 'security', 'vulnerability', 'fix', 'hotfix', 'patch', 'critical']
            label_matches = [label for label in labels_lower if any(bug_label in label for bug_label in bug_labels)]

            # æ£€æŸ¥fixå¼•ç”¨æ¨¡å¼
            fix_patterns = [
                r'fix(?:es)?\s*#?\d+',  # fixes #123
                r'resolv(?:es)?\s*#?\d+',  # resolves #123
                r'clos(?:es)?\s*#?\d+',  # closes #123
                r'fix(?:es)?\s+\w+',  # fixes bug
                r'patch(?:es)?\s+\w+',  # patches issue
            ]
            fix_references = []
            for pattern in fix_patterns:
                fix_references.extend(re.findall(pattern, title_body_text))

            # Rocket Poolç‰¹å®šçš„bugæ¨¡å¼
            rocketpool_bug_patterns = [
                # è´¨æŠ¼ç›¸å…³bug
                r'staking.*(?:fail|error|bug|incorrect|revert)',
                r'validator.*(?:fail|error|bug|wrong|invalid|exit)',
                r'minipool.*(?:fail|error|bug|stuck|dissolve|finalis)',
                r'deposit.*(?:fail|error|bug|insufficient|excess|lost)',
                r'withdrawal.*(?:fail|error|bug|delay|stuck|timeout)',
                r'node.*(?:fail|error|bug|offline|sync|disconnect)',

                # ä»£å¸å’Œå¥–åŠ±bug
                r'reth.*(?:fail|error|bug|mint|burn|ratio|exchange)',
                r'rpl.*(?:fail|error|bug|stake|unstake|slash|lock)',
                r'reward.*(?:fail|error|bug|claim|distribute|calculate)',
                r'commission.*(?:fail|error|bug|rate|calculation|split)',
                r'smoothing.*(?:fail|error|bug|pool|interval|merkle)',
                r'claim.*(?:fail|error|bug|proof|merkle|tree|verify)',

                # æ²»ç†å’ŒOracle bug
                r'dao.*(?:fail|error|bug|vote|proposal|execute)',
                r'governance.*(?:fail|error|bug|settings|parameter)',
                r'oracle.*(?:fail|error|bug|price|ratio|feed|update)',
                r'guardian.*(?:fail|error|bug|upgrade|pause|emergency)',
                r'trusted.*(?:fail|error|bug|node|consensus|vote)',

                # ç½‘ç»œå’ŒåŒæ­¥bug
                r'beacon.*(?:fail|error|bug|chain|sync|slot|epoch)',
                r'consensus.*(?:fail|error|bug|layer|client|fork)',
                r'execution.*(?:fail|error|bug|layer|payload|block)',
                r'sync.*(?:fail|error|bug|committee|attestation)',
                r'epoch.*(?:fail|error|bug|transition|boundary)',

                # å­˜å‚¨å’ŒçŠ¶æ€bug
                r'storage.*(?:fail|error|bug|corruption|inconsistent)',
                r'state.*(?:fail|error|bug|transition|invalid|corrupt)',
                r'checkpoint.*(?:fail|error|bug|save|restore|missing)',
                r'queue.*(?:fail|error|bug|overflow|underflow|stuck)',

                # å®‰å…¨ç›¸å…³bug
                r'slashing.*(?:fail|error|bug|penalty|calculation)',
                r'penalty.*(?:fail|error|bug|excessive|insufficient)',
                r'access.*(?:fail|error|bug|control|permission|unauthorized)',
                r'upgrade.*(?:fail|error|bug|proxy|implementation)',
                r'reentrancy.*(?:fail|error|bug|attack|guard)',

                # Gaså’Œæ€§èƒ½bug
                r'gas.*(?:fail|error|bug|limit|optimization|expensive)',
                r'timeout.*(?:fail|error|bug|delay|stuck|infinite)',
                r'deadlock.*(?:fail|error|bug|stuck|infinite|loop)',
                r'performance.*(?:fail|error|bug|slow|optimization)',

                # æ•°å­¦å’Œè®¡ç®—bug
                r'calculation.*(?:fail|error|bug|overflow|underflow|precision)',
                r'ratio.*(?:fail|error|bug|exchange|rate|incorrect)',
                r'balance.*(?:fail|error|bug|mismatch|inconsistent)',
                r'supply.*(?:fail|error|bug|mint|burn|total|circulation)'
            ]

            rocketpool_bug_matches = []
            for pattern in rocketpool_bug_patterns:
                rocketpool_bug_matches.extend(re.findall(pattern, title_body_text))

            # è®¡ç®—åŒ¹é…åˆ†æ•°ï¼ˆä¸åŸç ”ç©¶æ–¹æ³•è®ºä¸€è‡´ï¼‰
            match_score = (len(general_keyword_matches) +
                           len(label_matches) +
                           len(fix_references) +
                           len(rocketpool_bug_matches))

            if general_keyword_matches or label_matches or fix_references or rocketpool_bug_matches:
                confidence = 'high' if match_score >= 3 else 'medium' if match_score >= 1 else 'low'

                bug_candidates.append({
                    **pr,
                    'general_keyword_matches': general_keyword_matches,
                    'rocketpool_keyword_matches': rocketpool_keyword_matches,
                    'label_matches': label_matches,
                    'fix_references': fix_references,
                    'rocketpool_bug_matches': rocketpool_bug_matches,
                    'match_score': match_score,
                    'confidence': confidence
                })

        print(f"âœ… ä» {len(merged_prs)} ä¸ªåˆå¹¶PRä¸­è¯†åˆ«å‡º {len(bug_candidates)} ä¸ªç–‘ä¼¼bugä¿®å¤PR")

        # æŒ‰ç½®ä¿¡åº¦åˆ†ç±»ç»Ÿè®¡
        high_confidence = len([c for c in bug_candidates if c['confidence'] == 'high'])
        medium_confidence = len([c for c in bug_candidates if c['confidence'] == 'medium'])
        low_confidence = len([c for c in bug_candidates if c['confidence'] == 'low'])

        print(f"   - é«˜ç½®ä¿¡åº¦: {high_confidence}")
        print(f"   - ä¸­ç½®ä¿¡åº¦: {medium_confidence}")
        print(f"   - ä½ç½®ä¿¡åº¦: {low_confidence}")

        # æŒ‰Rocket PoolåŠŸèƒ½åˆ†ç±»ç»Ÿè®¡
        staking_bugs = len(
            [c for c in bug_candidates if any('staking' in match or 'validator' in match or 'minipool' in match
                                              for match in
                                              c['rocketpool_keyword_matches'] + c['rocketpool_bug_matches'])])
        reward_bugs = len([c for c in bug_candidates if any('reward' in match or 'reth' in match or 'smoothing' in match
                                                            for match in c['rocketpool_keyword_matches'] + c[
                                                                'rocketpool_bug_matches'])])
        governance_bugs = len(
            [c for c in bug_candidates if any('dao' in match or 'governance' in match or 'oracle' in match
                                              for match in
                                              c['rocketpool_keyword_matches'] + c['rocketpool_bug_matches'])])
        security_bugs = len(
            [c for c in bug_candidates if any('slashing' in match or 'guardian' in match or 'access' in match
                                              for match in
                                              c['rocketpool_keyword_matches'] + c['rocketpool_bug_matches'])])

        print(f"   - Stakingç›¸å…³bug: {staking_bugs}")
        print(f"   - Rewardç›¸å…³bug: {reward_bugs}")
        print(f"   - Governanceç›¸å…³bug: {governance_bugs}")
        print(f"   - Securityç›¸å…³bug: {security_bugs}")

        return bug_candidates

    def export_results(self, merged_prs, bug_candidates, stats):
        """å¯¼å‡ºç»“æœåˆ°Excel"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿ç›®å½•åˆ›å»º
        excel_dir = os.path.abspath(ROCKETPOOL_CONFIG['excel_output'])
        os.makedirs(excel_dir, exist_ok=True)

        excel_file = os.path.join(excel_dir, f"rocket_pool_{timestamp}.xlsx")

        print(f"ğŸ“‚ æ­£åœ¨åˆ›å»ºExcelæ–‡ä»¶...")
        print(f"   ç›®å½•: {excel_dir}")
        print(f"   æ–‡ä»¶: rocket_pool_{timestamp}.xlsx")

        try:
            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                # 1. æ‰€æœ‰åˆå¹¶çš„PR
                merged_df = pd.DataFrame(merged_prs)
                merged_df.to_excel(writer, sheet_name='All_Merged_PRs', index=False)

                # 2. ç–‘ä¼¼bugä¿®å¤PR
                if bug_candidates:
                    bug_df = pd.DataFrame(bug_candidates)
                    # é€‰æ‹©é‡è¦åˆ—
                    bug_display_df = bug_df[[
                        'number', 'title', 'user', 'merged_at', 'match_score', 'confidence',
                        'general_keyword_matches', 'rocketpool_keyword_matches', 'label_matches',
                        'project_name', 'project_type', 'project_domain', 'url'
                    ]].copy()

                    # æ ¼å¼åŒ–åŒ¹é…ç»“æœ
                    bug_display_df['general_keyword_matches'] = bug_display_df['general_keyword_matches'].apply(
                        lambda x: ', '.join(x[:5]))
                    bug_display_df['rocketpool_keyword_matches'] = bug_display_df['rocketpool_keyword_matches'].apply(
                        lambda x: ', '.join(x[:5]))
                    bug_display_df['label_matches'] = bug_display_df['label_matches'].apply(lambda x: ', '.join(x))

                    bug_display_df.to_excel(writer, sheet_name='Bug_Fix_Candidates', index=False)

                # 3. ç»Ÿè®¡ä¿¡æ¯
                stats_data = [
                    ['é¡¹ç›®åç§°', 'Rocket Pool'],
                    ['é¡¹ç›®ç±»å‹', 'DeFi'],
                    ['é¡¹ç›®é¢†åŸŸ', 'Ethereum Staking Protocol'],
                    ['ä»“åº“åœ°å€', f"{self.owner}/{self.repo}"],
                    ['æ€»åˆå¹¶PRæ•°', stats['total_prs']],
                    ['Stakingç›¸å…³PRæ•°', stats['staking_prs']],
                    ['Rewardç›¸å…³PRæ•°', stats['reward_prs']],
                    ['Governanceç›¸å…³PRæ•°', stats['governance_prs']],
                    ['Securityç›¸å…³PRæ•°', stats['security_prs']],
                    ['ç–‘ä¼¼bugä¿®å¤PRæ•°', len(bug_candidates)],
                    ['æœ€æ´»è·ƒè´¡çŒ®è€…', stats['user_counts'].index[0] if not stats['user_counts'].empty else 'N/A'],
                    ['æ€»ä»£ç å¢åŠ è¡Œæ•°', stats['code_stats']['additions']],
                    ['æ€»ä»£ç åˆ é™¤è¡Œæ•°', stats['code_stats']['deletions']],
                    ['æ€»å˜æ›´æ–‡ä»¶æ•°', stats['code_stats']['files']]
                ]

                stats_df = pd.DataFrame(stats_data, columns=['æŒ‡æ ‡', 'æ•°å€¼'])
                stats_df.to_excel(writer, sheet_name='Statistics', index=False)

                # 4. æ—¶é—´è¶‹åŠ¿
                time_df = stats['date_counts'].reset_index()
                time_df.columns = ['æ—¥æœŸ', 'PRæ•°é‡']
                time_df.to_excel(writer, sheet_name='Time_Trends', index=False)

                # 5. ç½®ä¿¡åº¦åˆ†å¸ƒ
                if bug_candidates:
                    confidence_counts = pd.Series([c['confidence'] for c in bug_candidates]).value_counts()
                    confidence_df = confidence_counts.reset_index()
                    confidence_df.columns = ['ç½®ä¿¡åº¦', 'æ•°é‡']
                    confidence_df.to_excel(writer, sheet_name='Confidence_Distribution', index=False)

                # 6. Rocket PoolåŠŸèƒ½åˆ†ç±»
                if bug_candidates:
                    function_data = []
                    for candidate in bug_candidates:
                        functions = []
                        matches = candidate['rocketpool_keyword_matches'] + candidate['rocketpool_bug_matches']

                        if any('staking' in match or 'validator' in match or 'minipool' in match for match in matches):
                            functions.append('Staking')
                        if any('reward' in match or 'reth' in match or 'smoothing' in match for match in matches):
                            functions.append('Reward')
                        if any('dao' in match or 'governance' in match or 'oracle' in match for match in matches):
                            functions.append('Governance')
                        if any('slashing' in match or 'guardian' in match or 'access' in match for match in matches):
                            functions.append('Security')

                        function_data.append({
                            'PR_Number': candidate['number'],
                            'Title': candidate['title'],
                            'Functions': ', '.join(functions) if functions else 'General',
                            'Confidence': candidate['confidence']
                        })

                    function_df = pd.DataFrame(function_data)
                    function_df.to_excel(writer, sheet_name='Function_Classification', index=False)

            # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„åˆ›å»ºæˆåŠŸ
            if os.path.exists(excel_file):
                file_size = os.path.getsize(excel_file)
                print(f"âœ… æ–‡ä»¶åˆ›å»ºæˆåŠŸï¼")
                print(f"   å¤§å°: {file_size:,} bytes")
            else:
                print(f"âŒ æ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼")

        except Exception as e:
            print(f"âŒ å¯¼å‡ºExcelæ—¶å‡ºé”™: {e}")
            excel_file = None

        print(f"ğŸ“ Rocket Poolç»“æœå·²å¯¼å‡ºåˆ°: {excel_file}")
        print(f"ğŸ“‚ å®Œæ•´è·¯å¾„: {os.path.abspath(excel_file) if excel_file else 'N/A'}")
        return excel_file

    def make_request(self, url, params=None):
        """å‘é€APIè¯·æ±‚"""
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 403:
                print("âš ï¸  APIé…é¢å¯èƒ½ä¸è¶³ï¼Œè¯·ç¨åé‡è¯•")
                return None
            else:
                print(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
        except Exception as e:
            print(f"è¯·æ±‚å¼‚å¸¸: {e}")
            return None

    def run_collection(self):
        """è¿è¡Œå®Œæ•´çš„æ”¶é›†æµç¨‹"""
        print("ğŸš€ å¼€å§‹æ”¶é›†Rocket Poolå·²åˆå¹¶çš„PR...")
        print("ğŸ“– å®éªŒæµç¨‹ï¼šä¸“é—¨åˆ†æSolidityæ™ºèƒ½åˆçº¦ä»“åº“")
        print("ğŸ”— é¡¹ç›®ï¼šRocket Pool - å»ä¸­å¿ƒåŒ–ä»¥å¤ªåŠè´¨æŠ¼åè®®")
        print(f"ğŸ“ ä»“åº“ï¼š{self.owner}/{self.repo}")

        # 1. æ”¶é›†æ‰€æœ‰å·²åˆå¹¶çš„PR
        merged_prs = self.collect_all_merged_prs()

        if not merged_prs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å·²åˆå¹¶çš„PR")
            return

        # 2. åˆ†æPRæ•°æ®
        stats = self.analyze_merged_prs(merged_prs)

        # 3. è¯†åˆ«bugä¿®å¤ç›¸å…³çš„PR
        bug_candidates = self.identify_bug_fix_prs(merged_prs)

        # 4. å¯¼å‡ºç»“æœ
        excel_file = self.export_results(merged_prs, bug_candidates, stats)

        print(f"\nâœ… Rocket Poolæ•°æ®æ”¶é›†å®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœæ‘˜è¦:")
        print(f"   - é¡¹ç›®: Rocket Pool")
        print(f"   - ç±»å‹: DeFi")
        print(f"   - é¢†åŸŸ: Ethereum Staking Protocol")
        print(f"   - æ€»åˆå¹¶PR: {len(merged_prs)}")
        print(f"   - StakingåŠŸèƒ½PR: {stats['staking_prs']}")
        print(f"   - RewardåŠŸèƒ½PR: {stats['reward_prs']}")
        print(f"   - GovernanceåŠŸèƒ½PR: {stats['governance_prs']}")
        print(f"   - SecurityåŠŸèƒ½PR: {stats['security_prs']}")
        print(f"   - ç–‘ä¼¼bugä¿®å¤: {len(bug_candidates)}")
        print(f"   - ç»“æœæ–‡ä»¶: {excel_file}")

        # æ˜¾ç¤ºé¡¹ç›®ç›®å½•ç»“æ„
        print(f"\nğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•ç»“æ„:")
        output_base = os.path.abspath('./output')
        if os.path.exists(output_base):
            for root, dirs, files in os.walk(output_base):
                level = root.replace(output_base, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f"{indent}{os.path.basename(root)}/")
                subindent = ' ' * 2 * (level + 1)
                for file in files:
                    print(f"{subindent}{file}")

        print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥:")
        print(f"   1. äººå·¥å®¡æ ¸ç–‘ä¼¼bugä¿®å¤PRåˆ—è¡¨")
        print(f"   2. ç¡®è®¤çœŸæ­£çš„bugä¿®å¤å®ä¾‹")
        print(f"   3. æŒ‰8ç§bugç±»å‹è¿›è¡Œåˆ†ç±»")
        print(f"   4. åˆ†æETHè´¨æŠ¼åè®®çš„ç‰¹æœ‰bugæ¨¡å¼")
        print(f"   5. é‡ç‚¹å…³æ³¨éªŒè¯å™¨ã€è´¨æŠ¼æ± ã€å¥–åŠ±åˆ†å‘ç­‰æ¨¡å—")


if __name__ == "__main__":
    collector = RocketPoolCollector()
    collector.run_collection()